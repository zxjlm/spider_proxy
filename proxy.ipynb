{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://59.44.247.194:9797\n",
      "http://124.207.82.166:8008\n",
      "https://211.147.239.101:57281\n",
      "http://163.125.67.145:9797\n",
      "http://211.162.70.229:3128\n",
      "http://112.85.169.82:9999\n",
      "http://125.126.208.81:9999\n",
      "http://112.85.168.92:9999\n",
      "https://163.204.244.100:9999\n",
      "https://61.142.72.154:30074\n",
      "https://180.164.24.165:53281\n"
     ]
    }
   ],
   "source": [
    "# 用于发送 HTTP 请求的库\n",
    "import requests\n",
    "# 用于解析 HTML 并提取信息\n",
    "from bs4 import BeautifulSoup\n",
    "# time 这个库主要用来记录一下时间\n",
    "import time\n",
    "# 在验证 ip 有效性的时候我会开多线程\n",
    "from threading import Thread\n",
    "'''\n",
    "队列用于线程间通信。需要注意的是，得从 queue 导入 Queue。\n",
    "不能从 multiprocessing 导入 Queue，这是用于进程间通信的。\n",
    "虽然两者用法基本一致，但底层实现不同。\n",
    "'''\n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "\n",
    "class GetIp:\n",
    "\n",
    "    def __init__(self, verify_site):\n",
    "        '''\n",
    "        类初始化。\n",
    "\n",
    "        :param str verify_site: 用于对代理ip进行测试的网站，可以填入要爬的网站链接。\n",
    "        '''\n",
    "        # proxy_list 保存从西刺爬下来的代理\n",
    "        self.proxy_list = []\n",
    "        # success_list 保存验证通过的代理\n",
    "        self.success_list = []\n",
    "        self.site = verify_site\n",
    "\n",
    "    def get_proxies(self):\n",
    "        '''\n",
    "        从西刺首页获取 http 和 https 代理地址并存入实例变量 proxy_list。\n",
    "        '''\n",
    "        # 记录开始时间\n",
    "        start = time.time()\n",
    "        print('start crawling ip...')\n",
    "        try:\n",
    "            # 发送http请求，加一个 User-Agent 的请求头让服务器觉得我们不像爬虫脚本，\n",
    "            # 不加 headers 也可以。\n",
    "            html = requests.get('http://www.xicidaili.com/', headers={\n",
    "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36'\n",
    "            })\n",
    "        except:\n",
    "            # 若请求失败则抛出异常，中断程序，因为获取不到网页内容，继续下去也没意义了。\n",
    "            raise\n",
    "        # 以下的解析部分的注释省去，各位看一下 BeautifulSoup 的文档就知道了，不是本文的重点。\n",
    "        soup = BeautifulSoup(html.text, 'lxml')\n",
    "        trs = soup.find('table', id='ip_list').find_all('tr', class_=['', 'odd'])[2:]\n",
    "        for tr in trs:\n",
    "            scheme = tr.findAll('td', class_=\"\")[-3].get_text().lower()\n",
    "            if scheme in ['http', 'https']:\n",
    "                ip = tr.findAll('td', class_=\"\")[0].get_text()\n",
    "                port = tr.findAll('td', class_=\"\")[1].get_text()\n",
    "                proxy = scheme + '://' + ip + ':' + port\n",
    "                self.proxy_list.append(proxy)\n",
    "        # 打印运行时间\n",
    "        print('[Get Proxies]total time: %.2f' % (time.time() - start))\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        负责获取代理，开启多线程任务对代理地址进行验证。\n",
    "        '''\n",
    "        # 先执行获取代理的函数，将爬下来的代理暂时保存至实例变量 proxy_list\n",
    "        self.get_proxies()\n",
    "        start = time.time()\n",
    "        # 实例化一个队列对象\n",
    "        proxy_q = Queue()\n",
    "        print('Verifying proxies...')\n",
    "        # workers 列表保存线程\n",
    "        workers = []\n",
    "        # 把爬下来的代理地址逐个放进队列\n",
    "        for p in self.proxy_list:\n",
    "            proxy_q.put(p)\n",
    "        # 开启15个线程并放入workers\n",
    "        # 最后往队列放入数量与线程一样的数字0，作为每个线程结束的标记\n",
    "        for _ in range(15):\n",
    "            workers.append(Thread(target=self.verify_proxies, args=(proxy_q,)))\n",
    "            proxy_q.put(0)\n",
    "        for w in workers:\n",
    "            # 启动线程\n",
    "            w.start()\n",
    "        for w in workers:\n",
    "            # 等待线程终止\n",
    "            w.join()\n",
    "        print('[verification] total time: %.2f s' % (time.time() - start))\n",
    "        print('proxies verified !')\n",
    "\n",
    "\n",
    "    def verify_proxies(self, proxy_q):\n",
    "        '''\n",
    "        验证代理有效性\n",
    "        :param list proxy_q: 待验证的代理队列\n",
    "        '''\n",
    "        # 写一个死循环不断从传入队列中获取代理地址\n",
    "        while 1:\n",
    "            proxy = proxy_q.get()\n",
    "            # 若获取到的值是 0，即退出循环，线程终止\n",
    "            if proxy == 0:  break\n",
    "            protocol = 'https' if 'https' in proxy else 'http'\n",
    "            proxies = {\n",
    "                protocol: proxy\n",
    "            }\n",
    "            try:\n",
    "                # 若返回状态码不等于200、或在HTTP连接过程中发生其他错误，则抛出异常\n",
    "                # 设置连接超时时间为3秒\n",
    "                # self.site为初始化时设置的测试链接\n",
    "                assert requests.get(self.site, timeout=3, proxies=proxies).status_code == 200\n",
    "            except:\n",
    "                print('[fail] %s' % proxy)\n",
    "            else:\n",
    "                print('[success] %s' % proxy)\n",
    "                # 保存验证通过的代理\n",
    "                self.success_list.append(proxy)\n",
    "                \n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    #print('Test Site: {}'.format('http://www.a-hospital.com/w'))\n",
    "    #g = GetIp('http://www.a-hospital.com/w')\n",
    "    #g.run()\n",
    "    with open('proxies1.txt', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            print(line.strip()) # 把末尾的'\\n'删掉\n",
    "\n",
    "        \n",
    "    \n",
    "#         for proxy in g.success_list:\n",
    "#             f.write(proxy + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
